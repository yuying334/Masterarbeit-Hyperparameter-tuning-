{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numba as nb\n",
    "import spacy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"does\"))\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',\n",
    "                 encoding='ISO-8859-1', \n",
    "                 names=[\n",
    "                        'target',\n",
    "                        'id',\n",
    "                        'date',\n",
    "                        'flag',\n",
    "                        'user',\n",
    "                        'text'\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df.target)\n",
    "df.target=le.transform(df.target)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.text.values\n",
    "y = df.target.values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[174])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_train[i] = re.sub(r'[^A-Za-z]+', ' ', x_train[i])\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_test[i] = re.sub(r'[^A-Za-z]+', ' ', x_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_data(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize the sentence and find the POS tag for each token\n",
    "    lemmatized_data=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        tagged_data=nltk.pos_tag(nltk.word_tokenize(data[i]))\n",
    "    # tuple of (token, wordnet_tag)\n",
    "        wordnet_tagged = map(lambda x: (\n",
    "            x[0], nltk_tag_to_wordnet_tag(x[1])), tagged_data)\n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in wordnet_tagged:\n",
    "            if tag is None:  # if there is no available tag, append the token as is\n",
    "                lemmatized_sentence.append(word)\n",
    "            else:# else use the tag to lemmatize the token\n",
    "                lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "        lemmatized_data.append(\" \".join(lemmatized_sentence))\n",
    "    return lemmatized_data       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train_lm=lemmatize_data(x_train)\n",
    "x_test_lm=lemmatize_data(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "vectorizer.fit(x_train_lm)\n",
    "x_train_td = vectorizer.transform(x_train_lm)\n",
    "x_test_td = vectorizer.transform(x_test_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_arr = np.arange(x_train_td.shape[0])\n",
    "data_to_svd=x_train_td[rand_arr[0:3000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_to_svd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = SVC(max_iter=2000,kernel='rbf',decision_function_shape='ovr')\n",
    "classifier.fit(x_train_td, y_train)\n",
    "\n",
    "score = classifier.score(x_test_td, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit = umap.UMAP(n_neighbors=20,\n",
    "                min_dist=0.1,\n",
    "                n_components=3,\n",
    "                metric='hellinger',\n",
    "                )\n",
    "rand_arr = np.arange(X_train.shape[0])\n",
    "X_train_ump=X_train[rand_arr[0:3000]]\n",
    "%time embedding = fit.fit_transform(X_train_ump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "svd = TruncatedSVD(n_components=200, n_iter=1000)\n",
    "data_svd = svd.fit_transform(data_to_svd)\n",
    "print (type(data_svd))\n",
    "print(data_svd.shape)\n",
    "\n",
    "#print(data_svd1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tsne = TSNE(\n",
    "    n_components=3,\n",
    "    n_iter=1000,\n",
    "    learning_rate=200,\n",
    "    perplexity=50,\n",
    "    random_state=1132,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "embedding = tsne.fit_transform(data_to_svd)\n",
    "\n",
    "embedding_df = pd.DataFrame(embedding, columns=[\"x\", \"y\", \"z\"])\n",
    "print(embedding_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "embedding_df.to_csv('embedding_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "scatter = go.Scatter3d(\n",
    "    name=str(embedding_df.index),\n",
    "    x=embedding_df[\"x\"],\n",
    "    y=embedding_df[\"y\"],\n",
    "    z=embedding_df[\"z\"],\n",
    "    text=x_train,\n",
    "    hoverinfo = 'text',\n",
    "    textposition=\"middle center\",\n",
    "    showlegend=False,\n",
    "    mode=\"markers\",\n",
    "    ids=y_train,\n",
    "    marker=dict(size=3, color=y_train, symbol=\"circle\"),\n",
    ")\n",
    "figure = go.Figure(data=[scatter])\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
